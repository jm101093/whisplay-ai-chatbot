# specify the ASR、LLM and TTS server to use
# options: OPENAI, GEMINI, TENCENT, VOLCENGINE, WHISPER, VOSK
ASR_SERVER=OPENAI 

# options: OPENAI, GEMINI, VOLCENGINE
# Note: For local inference with vLLM, use OPENAI and configure OPENAI_API_BASE_URL
LLM_SERVER=OPENAI 

# options: OPENAI, GEMINI, TENCENT, VOLCENGINE, PIPER
TTS_SERVER=OPENAI

# specify the image generation server to use, the generated images will be saved in the data/images folder
# options: OPENAI, GEMINI, VOLCENGINE
IMAGE_GENERATION_SERVER=OPENAI

# specify the chat history reset time in seconds, default is 5 minutes (300 seconds)
# CHAT_HISTORY_RESET_TIME=300

# if you want to clean the data folder on each start, set the following environment variable to true
# CLEAN_DATA_FOLDER_ON_START=true

# enable or disable thinking of LLM
# if you are using ollama as LLM server, please confirm that the model supports thinking before enabling this option
# otherwise the ollama will return a 400 error
ENABLE_THINKING=true


## Tencent Cloud ASR and TTS
# if you are using tencent cloud as ASR or TTS server, please set the following environment variables
TENCENT_SECRET_ID=YourSecretId
TENCENT_SECRET_KEY=YourSecretKey
# endpoint is optional, default is asr.tencentcloudapi.com for ASR and tts.tencentcloudapi.com for TTS
# TENCENT_ASR_ENDPOINT=asr.tencentcloudapi.com
# TENCENT_TTS_ENDPOINT=tts.tencentcloudapi.com

## ByteDance VolcEngine ASR and TTS

# if you are using volcengine as ASR or TTS server, please set the following environment variables
# VOLCENGINE_APP_ID=volcengine_app_id
VOLCENGINE_ACCESS_TOKEN=volcengine_access_token

# You can choose different voice types and LLM models, please refer to the official documentation for more details:
# https://www.volcengine.com/docs/6561/1257544
# VOLCENGINE_VOICE_TYPE=zh_female_wanwanxiaohe_moon_bigtts

## ByteDance Doubao LLM
# if you are using volcengine as LLM server, please set the following environment variables
VOLCENGINE_DOUBAO_ACCESS_TOKEN=volcengine_doubao_access_token

# the default model is doubao-1-5-lite-16k-250115, you can also choose other models
# VOLCENGINE_DOUBAO_LLM_MODEL=doubao-1-5-lite-32k-250115
# for image generation, the default model is doubao-seedream-3-0-t2i-250415, you can also choose other models
# VOLCENGINE_DOUBAO_IMAGE_MODEL=doubao-seedream-3-0-t2i-250415

## Google Gemini
# if you are using google gemini as ASR / LLM / TTS / IMAGE_GENERATION server, please set the following environment variables
# Google Cloud has 300$ free credit for new users, you can use it to try the gemini api
# get your API key from https://console.cloud.google.com/apis/credentials, and make sure to enable the Generative Language API for your project
# also ensure the API key has the permission to access the Generative Language API
GEMINI_API_KEY=your_api_key
# the default model is gemini-1.5-flash (for ASR and LLM), you can also choose other models
# GEMINI_MODEL=gemini-1.5-flash
# for TTS, the default voice is "gemini-2.5-pro-preview-tts", you can also choose other voices, please refer to: https://ai.google.dev/gemini-api/docs/speech-generation
# GEMINI_TTS_MODEL=gemini-2.5-flash-preview-tts
# the default speaker is "Callirrhoe", you can also choose other speakers, please refer to:https://ai.google.dev/gemini-api/docs/speech-generation#voices
# GEMINI_TTS_SPEAKER=Callirrhoe
# the default language code is "en-US", you can also choose other language codes, please refer to https://ai.google.dev/gemini-api/docs/speech-generation#languages
# GEMINI_TTS_LANGUAGE_CODE=en-US

## Gemini Image Generation
# for image generation, the default model is "gemini-2.5-flash-image", you can also choose other models, please refer to: https://ai.google.dev/gemini-api/docs/image-generation
GEMINI_IMAGE_MODEL=gemini-2.5-flash-image

## OpenAI
# if you are using openai as ASR、TTS or LLM server, please set the following environment variables
OPENAI_API_KEY=openai_api_key
# the default model is gpt-4o, you can also choose other models
# OPENAI_LLM_MODEL=gpt-4o
# if you are using other openai compatible api server (like vLLM), please set the following environment variables
# OPENAI_API_BASE_URL=http://localhost:8000/v1
# if you are using openai as image generation server, please specify the generation model below. The default model is "dall-e-3"
# if you want to use "gpt-image-1", your organization need to be verified by openai first.
# Document link: https://platform.openai.com/docs/guides/image-generation
OPENAI_IMAGE_MODEL=dall-e-3

## vLLM (Local LLM Inference)
# vLLM provides OpenAI-compatible API for local model inference
# Installation: bash install_vllm.sh
# GitHub: https://github.com/vllm-project/vllm
# To use vLLM, set LLM_SERVER=OPENAI and configure the following:
# OPENAI_API_BASE_URL=http://localhost:8000/v1
# OPENAI_API_KEY=token-abc123  # Can be any string for local vLLM
# OPENAI_LLM_MODEL=your-model-name  # Should match the model you're serving
# 
# Auto-start vLLM server when chatbot starts (optional):
# SERVE_VLLM=true
# VLLM_MODEL_PATH=/path/to/your/model  # e.g., ~/models/qwen2.5-0.5b
# VLLM_HOST=0.0.0.0
# VLLM_PORT=8000
# VLLM_MAX_MODEL_LEN=2048  # Adjust based on your RAM (lower = less memory)
# VLLM_GPU_MEMORY_UTILIZATION=0.9  # For GPU, adjust if needed
# Additional vLLM arguments (optional):
# VLLM_EXTRA_ARGS="--dtype float16 --quantization awq"

## llama.cpp (Local LLM Inference - Alternative to vLLM)
# llama.cpp provides OpenAI-compatible API for local GGUF model inference
# Recommended for lower-resource devices or when using quantized GGUF models
# Installation: bash install_llama_cpp.sh
# GitHub: https://github.com/ggerganov/llama.cpp
# To use llama.cpp, set LLM_SERVER=OPENAI and configure the following:
# OPENAI_API_BASE_URL=http://localhost:8080/v1
# OPENAI_API_KEY=token-abc123  # Can be any string for local llama.cpp
# OPENAI_LLM_MODEL=your-model-name  # Can be any string, llama.cpp ignores this
# 
# llama.cpp server configuration:
# LLAMA_CPP_MODEL_PATH=/path/to/your/model.gguf  # e.g., ~/models/qwen2.5-0.5b-q4_k_m.gguf
# LLAMA_CPP_HOST=127.0.0.1
# LLAMA_CPP_PORT=8080
# LLAMA_CPP_CONTEXT_SIZE=2048  # Context window size (lower = less memory)
# LLAMA_CPP_THREADS=4  # Number of CPU threads (default: all cores)
# LLAMA_CPP_BATCH_SIZE=512  # Batch size for prompt processing
# LLAMA_CPP_N_GPU_LAYERS=0  # Number of layers to offload to GPU (0 = CPU only)
# Additional llama.cpp arguments (optional):
# LLAMA_CPP_EXTRA_ARGS="--mlock --no-mmap"

## Proxy Settings
# proxy settings for all cloud api request, uncomment the following lines if you need to use a proxy to access the internet
# usually you only need to set HTTPS_PROXY
# HTTPS_PROXY=https://your_https_proxy
# HTTP_PROXY=http://your_http_proxy
# ALL_PROXY=socks5://your_socks_proxy

## Custom System Prompt
# you can set a custom system prompt for LLM, please uncomment the following line and set your own prompt
# SYSTEM_PROMPT="You are a happy girl and also a helpful assistant. Answer the question quickly in less than 5 sentences, also with a sense of humor."

## Piper TTS
# if you are using piper as TTS server, please set the following environment variables
# Piper is an open-source TTS engine that can run locally on your device
# You can download Piper binaries for RPi from: https://github.com/OHF-Voice/piper1-gpl/blob/387ca06bfd0e7557c6d0d54ce34d36e7bb28389a/docs/CLI.md
# Voice models can be found at: https://rhasspy.github.io/piper-samples/
PIPER_BINARY_PATH=/path/to/piper-binary
PIPER_MODEL_PATH=/path/to/piper/voice/model.onnx

## Coqui TTS
# if you are using Coqui TTS as TTS server, please set the following environment variables
# Coqui TTS is an open-source deep learning toolkit for Text-to-Speech synthesis
# Installation: pip install TTS
# GitHub: https://github.com/coqui-ai/TTS
# List available models: python python/coqui_tts.py --list-models
# The default model is "tts_models/en/ljspeech/tacotron2-DDC"
# Other recommended models:
#   - tts_models/en/vctk/vits (multi-speaker English)
#   - tts_models/multilingual/multi-dataset/your_tts (multi-lingual)
#   - tts_models/en/ljspeech/glow-tts (faster inference)
COQUI_TTS_MODEL=tts_models/en/ljspeech/tacotron2-DDC
# For multi-speaker models, specify the speaker name (optional)
# COQUI_TTS_SPEAKER=p225
# For multi-lingual models, specify the language code (optional)
# COQUI_TTS_LANGUAGE=en
# Enable GPU acceleration if available (requires CUDA)
# COQUI_TTS_GPU=false
# Python command to use (optional, default: python3)
# PYTHON_COMMAND=python3

## Vosk ASR
# if you are using vosk as ASR server, please set the following environment variables
# Vosk is an open-source ASR engine that can run locally on your device
# You can download Vosk binaries and models from: https://alphacephei.com/vosk/models
# unzip the model to a folder and set the path below
VOSK_MODEL_PATH=/path/to/vosk/model

## Whisper ASR (using faster-whisper)
# if you are using whisper as ASR server, please set the following environment variables
# Faster-whisper is an optimized implementation of Whisper using CTranslate2
# It provides up to 4x faster transcription with lower memory usage
# Installation: pip install faster-whisper
# GitHub: https://github.com/SYSTRAN/faster-whisper
# the default model size is tiny, you can also choose other model sizes: tiny, base, small, medium, large-v3
# Note: The model will be automatically downloaded on first use and cached locally
WHISPER_MODEL_SIZE=tiny
# the default language is auto-detect, you can also specify a language code (e.g., 'en', 'zh', 'ja')
# WHISPER_LANGUAGE=en
# Device to use for inference: cpu, cuda, or auto (default: cpu)
# WHISPER_DEVICE=cpu
# Compute type for inference: int8, int8_float16, float16 (default: int8)
# int8 is fastest and uses least memory, float16 is most accurate
# WHISPER_COMPUTE_TYPE=int8
